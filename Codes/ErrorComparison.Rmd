# Comparing 2 types of error metrics

MAE vs RMSE

```{r}
library(caret)
library(dplyr)
library(Metrics)

set.seed(123) # Pour la reproductibilité

# Importer le modèle delta
delta_model <- readRDS("../Models/delta_model.rds")

folds <- createFolds(model_data$Density, k = 10) # Création des plis

# Fonction pour la validation croisée et le calcul des métriques
cross_val <- function(train_index, test_index, data, delta_model) {
  train_data <- data[train_index, ]
  test_data <- data[test_index, ]
  
  # Vraies valeurs
  true_values <- test_data$Density
  
  # Entraînement du modèle sur le sous-ensemble d'apprentissage
  trained_model <- train_delta_model(train_data, "Density")
  
  # Prédictions sur l'ensemble de test
  predictions <- predict_delta_model(trained_model, test_data %>% select(-Density))
  
  # Calcul des métriques
  rmse_val <- rmse(true_values, predictions)
  mae_val <- mae(true_values, predictions)
  
  # Renvoyer les métriques
  return(list(RMSE = rmse_val, MAE = mae_val))
}

# Appliquer la validation croisée et calculer les erreurs
errors <- lapply(seq_along(folds), function(i) {
  train_index <- folds[[i]]
  test_index <- setdiff(seq_len(nrow(model_data)), train_index)
  cross_val(train_index, test_index, model_data, delta_model)
})

# Calculer la moyenne des erreurs
mean_rmse <- mean(sapply(errors, `[[`, "RMSE"))
mean_mae <- mean(sapply(errors, `[[`, "MAE"))

# Afficher les résultats
print(paste("RMSE moyen:", mean_rmse))
print(paste("MAE moyen:", mean_mae))

# Répéter pour les données non-zéro
non_zero_data <- subset(model_data, Density > 0)
non_zero_folds <- createFolds(non_zero_data$Density, k = 10)
non_zero_errors <- lapply(seq_along(non_zero_folds), function(i) {
  train_index <- non_zero_folds[[i]]
  test_index <- setdiff(seq_len(nrow(non_zero_data)), train_index)
  cross_val(train_index, test_index, non_zero_data, delta_model)
})

mean_rmse_non_zero <- mean(sapply(non_zero_errors, `[[`, "RMSE"))
mean_mae_non_zero <- mean(sapply(non_zero_errors, `[[`, "MAE"))

# Afficher les résultats pour les données non-zéro
print(paste("RMSE moyen (non-zéro):", mean_rmse_non_zero))
print(paste("MAE moyen (non-zéro):", mean_mae_non_zero))
```

A version to compute with range

```{r}
# Calcul de la plage des valeurs de densité
range_density <- range(model_data$Density, na.rm = TRUE)
range_density_value <- range_density[2] - range_density[1]

# CVRMSE et Coefficient de Variation de la MAE (CVMAE)
cvrmse <- mean_rmse / range_density_value
cvmae <- mean_mae / range_density_value

print(paste("CVRMSE:", cvrmse))
print(paste("CVMAE:", cvmae))

# Calculs similaires pour les données non-zéro
range_density_non_zero <- range(non_zero_data$Density, na.rm = TRUE)
range_density_non_zero_value <- range_density_non_zero[2] - range_density_non_zero[1]

cvrmse_non_zero <- mean_rmse_non_zero / range_density_non_zero_value
cvmae_non_zero <- mean_mae_non_zero / range_density_non_zero_value

print(paste("CVRMSE (non-zéro):", cvrmse_non_zero))
print(paste("CVMAE (non-zéro):", cvmae_non_zero))
```

-   **CVRMSE de 0.0858**: Cela signifie que la racine de l'erreur quadratique moyenne (RMSE) est équivalente à 8.58% de la plage des valeurs de densité de poissons. En d'autres termes, en moyenne, les erreurs de prévision de ton modèle sont de 8.58% de l'intervalle total entre la plus petite et la plus grande densité observée.

-   **CVMAE de 0.0303**: Ceci indique que l'erreur absolue moyenne (MAE) est équivalente à 3.03% de la plage des valeurs de densité de poissons. Cela signifie que, en moyenne, les erreurs absolues (sans considération de la direction de l'erreur) représentent 3.03% de la plage totale des valeurs observées.

Lorsque tu compares les versions "tout inclus" et "non-zéro", tu peux voir que tant le CVRMSE que le CVMAE sont plus élevés pour les données non nulles. Cela indique que ton modèle a plus de difficultés à prédire les valeurs quand elles ne sont pas nulles, ce qui pourrait être attendu dans des données écologiques où les zéros sont fréquents et où prédire des occurrences (non-zéros) peut être plus difficile que de prédire l'absence (zéros).

-   **CVRMSE (non-zéro) de 0.1380**: Lorsqu'on ne considère que les observations non nulles, la RMSE normalisée augmente à 13.8% de la plage des valeurs observées pour les densités de poissons non nulles.

-   **CVMAE (non-zéro) de 0.0802**: De même, la MAE normalisée pour les observations non nulles est de 8.02% de la plage de densité non nulle.

Ces résultats indiquent que le modèle est relativement bon pour prédire les valeurs y compris les zéros (car le CVRMSE et le CVMAE sont relativement bas), mais il est moins précis lorsqu'il s'agit uniquement des valeurs de densité positives (non-zéros), où les erreurs représentent une plus grande proportion de la plage des valeurs observées. Cela pourrait être un point de départ pour examiner comment le modèle gère les prédictions de faibles densités de poissons et pour explorer des stratégies pour améliorer ces prédictions, telles que le rééquilibrage des données, l'ajustement des paramètres du modèle ou l'utilisation de techniques de modélisation différentes.

------------------------------------------------------------------------

## Positive model post and pre log comaprison

Before doing so we must rebuild the old version of the delta model :\

```{r}
train_delta_model_old <- function(data, target, save_model = FALSE, model_name = "delta_model_old"){
  library(xgboost)
  library(dplyr)
  
  # Ensure the target column is a character string
  # target <- deparse(substitute(target))
  
  # Selecting only the numeric variables and removing the target column for the logistic model
  numeric_data <- data %>%
    select_if(is.numeric) %>%
    select(-all_of(target))
  
  # Creating the target for the logistic model
  logistic_target <- ifelse(test = data[[target]]==0, 0, 1)
  
  # Creating the dataset for the Presence/Absence model, ensuring 'Density' is excluded
  positive_data <- data %>% 
    filter(!!as.name(target) > 0) %>% 
    select_if(is.numeric) %>%
    select(-all_of(target))
  
  # Creating the positive target
  positive_target <- data %>% 
    filter(!!as.name(target) > 0) %>% 
    pull(!!as.name(target))
  
  # Defining the dmatrix object for xgb.train method for the logistic model
  dmatrix_logistic <- xgb.DMatrix(
    data = as.matrix(numeric_data),
    label = logistic_target
  )
  
  # Defining the parameters for the logistic model
  params_logistic <- list(
    booster = "gbtree",
    objective = "binary:logistic",
    eta = 0.05,
    gamma = 0.1,
    max_depth = 6,
    min_child_weight = 3,
    subsample = 0.75,
    colsample_bytree = 1
  )
  
  # Training the logistic model
  logistic_model <- xgb.train(
    params = params_logistic,
    data = dmatrix_logistic,
    nrounds = 200
  )
  
  # Parameters for the positive model
  params_positive <- list(
    booster = "gbtree",
    objective = "reg:squarederror",
    eta = 0.05,
    gamma = 0.1,
    max_depth = 6,
    min_child_weight = 3,
    subsample = 0.75,
    colsample_bytree = 1
  )
  
  # Defining the dmatrix object for xgb.train method for the positive model
  dmatrix_positive <- xgb.DMatrix(
    data = as.matrix(positive_data),
    label = positive_target
  )
  
  # Training the positive model
  positive_model <- xgb.train(
    params = params_positive,
    data = dmatrix_positive,
    nrounds = 200
  )
  
  # Include the feature names in the return object
  model_features <- names(numeric_data)
  
  if(save_model == TRUE){
    saveRDS(
      list(
        logistic_model = logistic_model,
        positive_model = positive_model,
        features = model_features
      ),
      paste0("../Models/", model_name, ".rds")
    )
  }
  
  # Return both models
  return(
    list(
      logistic_model = logistic_model,
      positive_model = positive_model,
      features = model_features
    )
  )
}
predict_delta_model_old <- function(model, newdata) {
  library(xgboost)
  library(dplyr)
  
  # Use the features stored in the model object
  model_features <- model$features
  
  # Ensure newdata contains only the features used during training
  newdata_numeric <- newdata %>% 
    select(all_of(model_features)) # Use model_features to select features
  
  # DMatrix object for the prediction
  dmatrix_newdata <- xgb.DMatrix(data = as.matrix(newdata_numeric))

  # Predict presence/absence using the logistic model
  presence_probabilities <- predict(model$logistic_model, dmatrix_newdata)
  
  # Initialize predictions vector
  final_predictions <- rep(0, nrow(newdata))
  
  # Predict with the positive model where presence is likely
  positive_indices <- which(presence_probabilities > 0.5)
  if (length(positive_indices) > 0) {
    positive_newdata <- newdata_numeric[positive_indices, , drop = FALSE]
    dmatrix_positive_newdata <- xgb.DMatrix(data = as.matrix(positive_newdata))
    positive_predictions <- predict(model$positive_model, dmatrix_positive_newdata)
    final_predictions[positive_indices] <- positive_predictions
  }
  
  return(final_predictions)
}

set.seed(123) # For reproductibility

delta_model_old <- train_delta_model_old(
  data = model_data,
  target = "Density",
  save_model = TRUE
)
```

Then we can do our stuff

```{r}
library(Metrics)

# Supposons que positive_model_with_log et positive_model_without_log sont tes deux modèles
# Et que test_data est déjà préparé et divisé en features et target

# Appliquer la validation croisée et calculer les erreurs
errors_old_model <- lapply(seq_along(folds), function(i) {
  train_index <- folds[[i]]
  test_index <- setdiff(seq_len(nrow(model_data)), train_index)
  cross_val(train_index, test_index, model_data, delta_model_old)
})

# Compute the rmse with the old model
mean_rmse_old <- mean(sapply(errors_old_model, `[[`, "RMSE"))

# Compute the range to have a percentage rmse
range_density <- range(model_data$Density, na.rm = TRUE)
range_density_value <- range_density[2] - range_density[1]

# Compute the perc rmse
rmse_with_log <- cvrmse
rmse_without_log <- mean_rmse_old / range_density_value

# Afficher les RMSE
cat("RMSE avec transformation log: ", rmse_with_log, "\n")
cat("RMSE sans transformation log: ", rmse_without_log, "\n")

```

Balbla

```{r}
library(dplyr)
library(xgboost)
library(Metrics)

set.seed(123)  # Pour la reproductibilité

# Préparation des données
model_data <- data_merlan %>% select(-c(Surface, Abundance))

# Création des plis pour la validation croisée
folds <- createFolds(model_data$Density, k = 10)

# Fonction pour effectuer la validation croisée et calculer les métriques pour les deux modèles
cross_val_comparison <- function(train_index, test_index, data) {
  train_data <- data[train_index, ]
  test_data <- data[test_index, ]
  
  # Entraînement et prédiction avec le modèle sans transformation log
  trained_model_old <- train_delta_model_old(train_data, "Density")
  predictions_old <- predict_delta_model_old(trained_model_old, test_data %>% select(-Density))
  
  # Entraînement et prédiction avec le modèle avec transformation log
  trained_model_new <- train_delta_model(train_data, "Density")
  predictions_new <- predict_delta_model(trained_model_new, test_data %>% select(-Density))
  
  # Calcul des métriques pour les deux modèles
  rmse_old <- rmse(test_data$Density, predictions_old)
  mae_old <- mae(test_data$Density, predictions_old)
  rmse_new <- rmse(test_data$Density, predictions_new)
  mae_new <- mae(test_data$Density, predictions_new)
  
  # Renvoyer les métriques
  return(list(
    RMSE_old = rmse_old,
    MAE_old = mae_old,
    RMSE_new = rmse_new,
    MAE_new = mae_new
  ))
}

# Appliquer la validation croisée et calculer les erreurs pour les deux configurations
errors_comparison <- lapply(seq_along(folds), function(i) {
  train_index <- folds[[i]]
  test_index <- setdiff(seq_len(nrow(model_data)), train_index)
  cross_val_comparison(train_index, test_index, model_data)
})

# Calculer la moyenne des erreurs pour chaque modèle
mean_rmse_old <- mean(sapply(errors_comparison, `[[`, "RMSE_old"))
mean_mae_old <- mean(sapply(errors_comparison, `[[`, "MAE_old"))
mean_rmse_new <- mean(sapply(errors_comparison, `[[`, "RMSE_new"))
mean_mae_new <- mean(sapply(errors_comparison, `[[`, "MAE_new"))

range = diff(range(model_data$Density))

# Afficher les résultats pour les deux modèles
cat("RMSE moyen (sans log):", mean_rmse_old/range, "\n")
cat("MAE moyen (sans log):", mean_mae_old/range, "\n")
cat("RMSE moyen (avec log):", mean_rmse_new/range, "\n")
cat("MAE moyen (avec log):", mean_mae_new/range, "\n")
```

## Error with and without extreme values :

```{r}
library(caret)
library(dplyr)
library(Metrics)

set.seed(123) # Pour la reproductibilité

# Préparer les données et exclure les valeurs de densité extrêmes
# model_data <- data_merlan %>%
#   select(-c(Surface, Abundance))

# Appliquer le filtre et vérifier le nombre de lignes
model_data_filtered <- model_data %>% filter(Density < 150)

# Assure-toi que 'folds' est créé à partir de 'model_data_filtered' et non de 'model_data'
folds <- createFolds(model_data_filtered$Density, k = 10)

# Appliquer la validation croisée et calculer les erreurs
errors <- lapply(seq_along(folds), function(i) {
  train_index <- folds[[i]]
  test_index <- setdiff(seq_len(nrow(model_data_filtered)), train_index)
  cross_val(train_index, test_index, model_data_filtered, delta_model)
})

# Calculer la moyenne des erreurs pour les densités < 150
mean_rmse_filtered <- mean(sapply(errors, `[[`, "RMSE"))
mean_mae_filtered <- mean(sapply(errors, `[[`, "MAE"))

# Calcul de la RMSE relative pour les données avec toutes les valeurs
range_full <- diff(range(model_data$Density, na.rm = TRUE))
rmse_relative_full <- mean_rmse_new / range_full

# Calcul de la RMSE relative pour les données sans les valeurs extrêmes
range_filtered <- diff(range(model_data_filtered$Density, na.rm = TRUE))
rmse_relative_filtered <- mean_rmse_filtered / range_filtered

# Affichage des RMSE relatives pour comparaison
print(paste("RMSE relative avec toutes les valeurs:", rmse_relative_full))
print(paste("RMSE relative pour densité < 150:", rmse_relative_filtered))
```
