# Modèle delta

```{r results="hide"}
# On charge les librairies
requiredPackages = c("xgboost", "caret", "dplyr")

for(pkge in requiredPackages) {
if (!require(pkge, character.only = TRUE))
install.packages(pkge)
library(pkge, character.only = TRUE)
}
```

# MODELE BINOMIAL

## Traitement de la variable Density

```{r}
set.seed(123) # For reproductibility

# Let's reset numeric_data here just in case it has been modified somewhere else
logistic_data = data_merlan %>% select_if(is.numeric)

# Remove the Year and Month columns
logistic_data = logistic_data %>% select(-c(Surface,Abundance))

# Do not forget to extract Desnity from the dataset
logistic_data_label = unlist(logistic_data$Density)
# Transforming the density into a binary variable
logistic_data_label_binary = ifelse(test = logistic_data_label==0,0,1)
# logistic_data_label_binary = as.factor(logistic_data_label_binary)

# Supress Density from the explanatory variables
logistic_data = logistic_data %>% select(-Density)
```

## BRT

Creating the `dcgMatrix` for the boosted regression tree method `xgboost`

```{r}
# Construction of the dcgMatrix object for the dataset

dmatrix_logistic <- xgb.DMatrix(data = as.matrix(logistic_data), label = logistic_data_label_binary)
```

### Hyperparamètres

On définit les paramètres du modèle :

```{r}
params <- list(
  booster = "gbtree",
  objective = "binary:logistic", # Utilisez "reg:squarederror" pour la régression
  eta = 0.3,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = 1
)
```

#### Validation croisée

Et maintenant on lance la validation croisée

```{r}
cv.nfold <- 5 # Nombre de plis pour la validation croisée
nrounds <- 200 # Nombre d'itérations de boosting

cv_results <- xgb.cv(
  params = params,
  data = dmatrix_logistic,
  nfold = cv.nfold,
  nrounds = nrounds,
  metrics = "rmse", # Utiliser "error" pour la classification, "rmse" pour la régression, etc.
  early_stopping_rounds = 10, # arrêt précoce
  stratified = TRUE, # si la classification, pour garder la même proportion de classes dans chaque pli
  seed = 123 # pour la reproductibilité
)
```

Affichage des résultats

```{r}
print(cv_results$evaluation_log)
```

```{r}
df_rmse_binom <- data.frame(cv_results$evaluation_log)
```

```{r}
par(mfrow=c(1,2))


# Graphe of the Decrease of the RMSE
graph_rmse_binom_mean <- ggplot(df_rmse_binom, aes(x = iter, y = test_rmse_mean)) +
  geom_line(color = "darkblue") +  
  labs(x = "Nombre d'itérations", y = "RMSE") +  
  ggtitle("Décroissance de la RMSE en fonction des itérations (échantillon de test)") +  
  theme_minimal()  # Thème minimal

# Afficher le graphique
graph_rmse_binom_mean


# -----------------------------------


# Graphe of the Decrease of the std's RMSE
graph_rmse_binom_std <- ggplot(df_rmse_binom, aes(x = iter, y = test_rmse_std)) +
  geom_line(color = "darkblue") +  
  labs(x = "Nombre d'itérations", y = "RMSE") +  
  ggtitle("Ecart-type de la RMSE en fonction des itérations (échantillon de test") +  
  theme_minimal()  # Thème minimal

# Afficher le graphique
graph_rmse_binom_std

par(mfrow=c(1,1))

```

#### Opti des hyperparam

Tentative d'optimisation des paramètres :

```{r}
grid <- expand.grid(
  nrounds = c(100, 200),
  eta = c(0.01, 0.05, 0.1),
  max_depth = c(3, 6, 9),
  gamma = c(0, 0.1, 0.2),
  colsample_bytree = c(0.5, 0.75, 1),
  min_child_weight = c(1, 3, 5),
  subsample = c(0.5, 0.75, 1)
)
```

Configuration du contrôle de la formation

```{r}
control <- trainControl(method = "cv", number = 5, search = "grid")
```

### Entrainement du BRT

Entraînement du modèle (Attention ce code est très long (env. 1h d'éxécution))

```{r}
# xgb_model <- train(
#   logistic_data,
#   logistic_data_label_binary,
#   trControl = control,
#   tuneGrid = grid,
#   method = "xgbTree"
# )
```

Showing the best model :

```{r}
xgb_model$bestTune
```

Le meilleur modèle est donc le suivant :

nrounds = 200

max_depth = 6

eta = 0.05

gamma = 0.1

colsample_bytree = 1

min_child_weight = 3

subsample = 0.75

------------------------------------------------------------------------

## Modèle binom avec best hyperparam

Let's create such a model, starting with the parameters grid

```{r}
params_logistic_model <- list(
  booster = "gbtree",
  objective = "binary:logistic", # Utilisez "reg:squarederror" pour la régression
  eta = 0.05,
  gamma = 0.1,
  max_depth = 6,
  min_child_weight = 3,
  subsample = 0.75,
  colsample_bytree = 1
)
```

Training the actual logistic model for presence/absence

```{r}
logistic_model <- xgb.train(
  params = params_logistic_model,
  data = dmatrix_logistic,
  nrounds = 200, # Supposons que vous avez choisi le nombre optimal de rounds basé sur xgb.cv
  # watchlist = list(eval = dmatrix_positive_model, train = dmatrix_positive_model),
  # early_stopping_rounds = 15
)

predictions_positive <- predict(positive_model, as.matrix(positive_density_model_data))
```

```         
        A FAIRE : 
        - afficher erreur du modèle binomial (s'inspirer de celui à densité) 
   
```

# MDELE DENSITE POSITIVE

Now onto the positive density model (second part of the delta model)

## Traitement de la variable Density

```{r}
# data for the positive density model
positive_density_model_data = data_merlan %>% select_if(is.numeric)

positive_density_model_data = positive_density_model_data %>%
  filter(Density > 0)

positive_density_model_data = positive_density_model_data %>% 
  select(-c(Abundance,Surface))

positive_density_model_label = unlist(positive_density_model_data$Density)

# Remove density from the explanatory variables
positive_density_model_data = positive_density_model_data %>% 
  select(-Density)

# Assert all the remaining data has positive density
all(positive_density_model_label > 0)
```

## Premier modèle

First BRT attempt

```{r}
# Construction of the dcgMatrix object for the dataset

dmatrix_positive_model <- xgb.DMatrix(
  data = as.matrix(positive_density_model_data),
  label = positive_density_model_label)
```

params definition

```{r}
params_positive_model <- list(
  booster = "gbtree",
  objective = "reg:squarederror", # Utilisez "reg:squarederror" pour la régression
  eta = 0.05,
  gamma = 0.1,
  max_depth = 6,
  min_child_weight = 3,
  subsample = 0.75,
  colsample_bytree = 1
)
```

### Validation croisée

Cross-Validation

```{r}
cv.nfold <- 5 # Nombre de plis pour la validation croisée
nrounds <- 200 # Nombre d'itérations de boosting

cv_results_positive_model <- xgb.cv(
  params = params_positive_model,
  data = dmatrix_positive_model,
  nfold = cv.nfold,
  nrounds = nrounds,
  metrics = "rmse", # Utiliser "error" pour la classification, "rmse" pour la régression, etc.
  early_stopping_rounds = 15, # arrêt précoce
  stratified = TRUE, # si la classification, pour garder la même proportion de classes dans chaque pli
  seed = 123 # pour la reproductibilité
)
```

Showing the results of the rmse of the cross-validation :

```{r}
print(cv_results_positive_model$evaluation_log)
```

```{r}
df_rmse_density <- data.frame(cv_results_positive_model$evaluation_log)
```

```{r}
par(mfrow=c(1,2))


# Graphe of the Decrease of the RMSE
graph_rmse_dens_mean <- ggplot(df_rmse_density, aes(x = iter, y = test_rmse_mean)) +
  geom_line(color = "darkblue") +  
  labs(x = "Nombre d'itérations", y = "RMSE") +  
  ggtitle("RMSE en fonction des itérations (échantillon de test)") +  
  theme_minimal()  # Thème minimal

# Afficher le graphique
graph_rmse_dens_mean


# -----------------------------------


# Graphe of the Decrease of the std's RMSE
graph_rmse_dens_std <- ggplot(df_rmse_density, aes(x = iter, y = test_rmse_std)) +
  geom_line(color = "darkblue") +  
  labs(x = "Nombre d'itérations", y = "RMSE") +  
  ggtitle("Ecart-type de la RMSE en fonction des itérations (échantillon de test") +  
  theme_minimal()  # Thème minimal

# Afficher le graphique
graph_rmse_dens_std

par(mfrow=c(1,1))

```

### Erreur du modèle à densité positive

Error visualisation:

```{r}
library(ggplot2)
```

Trying something visual

```{r}
positive_model <- xgb.train(
  params = params_positive_model,
  data = dmatrix_positive_model,
  nrounds = 200, # Supposons que vous avez choisi le nombre optimal de rounds basé sur xgb.cv
  watchlist = list(eval = dmatrix_positive_model, train = dmatrix_positive_model),
  early_stopping_rounds = 15
)

# Prédire en utilisant le modèle final
predictions_positive <- predict(positive_model, as.matrix(positive_density_model_data))
```

Onto the visual thing :

```{r}
# Créer un dataframe avec les prédictions et les valeurs réelles
results_df <- data.frame(
  Real_Density = positive_density_model_label,
  Predicted_Density = predictions_positive
)

# Afficher les premières lignes du dataframe pour vérification
head(results_df)
```

```{r}
# create a graphe with ggplot2
graphique <- ggplot(results_df, aes(x = results_df$Real_Density, y = results_df$Predicted_Density)) +
  geom_point(shape = 4, color = "lightblue") +  
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +  
  labs(x = "Valeurs observées", y = "Prédictions") +  
  ggtitle("Comparaison des valeurs observées et des prédictions") +  
  theme_minimal()  # Thème minimal

# Afficher le graphique
print(graphique)
```

Error positiv density model

```{r}
rmse_dens_pos <- sqrt(mean(results_df$Real_Density - results_df$Predicted_Density)^2)

rmse_dens_pos
```

# MODELE DELTA

## Functions !

Creating the delta model !

```{r}
library(xgboost)
library(dplyr)

train_delta_model <- function(data, target, nrounds = 200, early_stop_rounds = 10, save_model = FALSE, model_name = "delta_model"){
  # Encodage one-hot de la variable catégorielle 'substrat'
  # data <- fastDummies::dummy_cols(data_initial, select_columns = "substrat", remove_first_dummy = TRUE, remove_selected_columns = TRUE)

  # Afficher le nom de la colonne cible pour débogage
  print(paste("Target column:", target))

  # Division en set d'entraînement et de validation
  set.seed(123)  # Assure la reproductibilité
  train_index <- createDataPartition(data[[target]], p = 0.8, list = FALSE)
  train_data <- data[train_index, ]
  validation_data <- data[-train_index, ]

  # Préparation des DMatrix pour le modèle logistique
  logistic_train_matrix <- xgb.DMatrix(data = as.matrix(select(train_data, -all_of(target))), label = ifelse(train_data[[target]] > 0, 1, 0))
  logistic_validation_matrix <- xgb.DMatrix(data = as.matrix(select(validation_data, -all_of(target))), label = ifelse(validation_data[[target]] > 0, 1, 0))
  logistic_watchlist <- list(train = logistic_train_matrix, eval = logistic_validation_matrix)

  # Paramètres pour le modèle logistique
  params_logistic <- list(
    booster = "gbtree",
    objective = "binary:logistic",
    eta = 0.05,
    gamma = 0.1,
    max_depth = 6,
    min_child_weight = 3,
    subsample = 0.75,
    colsample_bytree = 1
  )

  # Entraînement du modèle logistique
  logistic_model <- xgb.train(
    data = logistic_train_matrix,
    params = params_logistic,
    nrounds = nrounds,
    early_stopping_rounds = early_stop_rounds,
    watchlist = logistic_watchlist,
    verbose = 1
  )

  # Préparation des données pour le modèle positif, appliquer la correction de Laurent
  positive_train_data <- filter(train_data, !!sym(target) > 0)
  positive_validation_data <- filter(validation_data, !!sym(target) > 0)
  positive_train_matrix <- xgb.DMatrix(data = as.matrix(select(positive_train_data, -all_of(target))), label = log1p(positive_train_data[[target]]))
  positive_validation_matrix <- xgb.DMatrix(data = as.matrix(select(positive_validation_data, -all_of(target))), label = log1p(positive_validation_data[[target]]))
  positive_watchlist <- list(train = positive_train_matrix, eval = positive_validation_matrix)

  # Paramètres pour le modèle positif
  params_positive <- list(
    booster = "gbtree",
    objective = "reg:squarederror",
    eta = 0.05,
    gamma = 0.1,
    max_depth = 6,
    min_child_weight = 3,
    subsample = 0.75,
    colsample_bytree = 1
  )

  # Entraînement du modèle positif
  positive_model <- xgb.train(
    data = positive_train_matrix,
    params = params_positive,
    nrounds = nrounds,
    early_stopping_rounds = early_stop_rounds,
    watchlist = positive_watchlist,
    verbose = 1
  )
  
  # Calcul de la variance pour le terme d'erreur de Laurent
positive_predictions_train <- predict(positive_model, positive_train_matrix)
variance_log_predictions <- var(positive_predictions_train)


  # Features utilisées
  features <- names(data %>% select(-target))

  # Sauvegarde du modèle si nécessaire
  if(save_model) {
    saveRDS(
      list(
        logistic_model = logistic_model,
        positive_model = positive_model,
        features = features,
        log_pred_variance = variance_log_predictions
      ),
      paste0("../Models/", model_name, ".rds")
    )
  }

  # Retour des modèles et des features
  return(
    list(
      logistic_model = logistic_model,
      positive_model = positive_model,
      features = features,
      log_pred_variance = variance_log_predictions
    )
  )
}
```

Now, let's create the **`predict_delta`**saveRDS( list(**`_model`** function. This function will accept a trained model (from **`train_delta_model`**) and new data to make predictions:

```{r}
predict_delta_model <- function(model, newdata) {
  library(xgboost)
  library(dplyr)
  
  # Use the features stored in the model object
  model_features <- model$features
  
  # Ensure newdata contains only the features used during training
  newdata_numeric <- newdata %>% 
    select(all_of(model_features)) # Use model_features to select features
  
  # DMatrix object for the prediction
  dmatrix_newdata <- xgb.DMatrix(data = as.matrix(newdata_numeric))

  # Predict presence/absence using the logistic model
  presence_probabilities <- predict(model$logistic_model, dmatrix_newdata)
  
  # Initialize predictions vector
  final_predictions <- rep(0, nrow(newdata))
  
  # Predict with the positive model where presence is likely
  positive_indices <- which(presence_probabilities > 0.5)
  if (length(positive_indices) > 0) {
    positive_newdata <- newdata_numeric[positive_indices, , drop = FALSE]
    dmatrix_positive_newdata <- xgb.DMatrix(data = as.matrix(positive_newdata))
    positive_predictions_log <- predict(model$positive_model, dmatrix_positive_newdata)
    # Correction de Laurent appliquée ici
    sigma_squared <- model$log_pred_variance  # récupération de la variance stockée
    correction_factor <- exp(sigma_squared / 2)
    positive_predictions <- expm1(positive_predictions_log) * correction_factor
    final_predictions[positive_indices] <- positive_predictions
  }

  return(final_predictions)
}
```

Here's a function to create the model data

```{r}
create_model_data <- function(data){
  library(dplyr)
  
  # Remove the unused columns
out = data_merlan %>%
  select(-c(Surface,Abundance,sali_mean,temp_mean,o2_mean,sali_cv,chl_mean,ID,Long,Lat,Year,Month,Origin,wave_mean,LogDensity))

out <- fastDummies::dummy_cols(out,
                               select_columns = "substrat",
                               remove_first_dummy = FALSE,
                               remove_selected_columns = TRUE
                               )

  return(out)
}
```

Then we train the model on the data

```{r}
set.seed(123) # For reproductibility

# Remove the Surface and Abundance columns
# model_data = data_merlan %>%
#   select(-c(Surface,Abundance,sali_mean,temp_mean,o2_mean,sali_cv,chl_mean,ID,Long,Lat,Year,Month,Origin,wave_mean)) %>% 
#   mutate(substrat_num = as.numeric(substrat)) %>% 
#   select(-substrat)
  
model_data = create_model_data(data_merlan)

delta_model <- train_delta_model(
  data = model_data,
  target = "Density",
  save_model = TRUE
)

delta_model = readRDS("../Models/delta_model.rds")
```

## Visualization

Just to visualize :

```{r}
df = cbind(data_merlan$Density,predictions_delta_model)

# Transform df in a real df 
df <- data.frame(df)
View(df)
```

Error visualisation:

```{r}
library(ggplot2)
```

```{r}
# Filtrer les données pour inclure uniquement les valeurs entre 150 et 200
df_filtered <- df[df$V1 >= 0 & df$V1 <= 150,]

# create a graphe with ggplot2
graphique <- ggplot(df_filtered, aes(x = V1, y = predictions_delta_model)) +
  geom_point(shape = 4, color = "darkblue") +  
  geom_abline(intercept = 0, slope = 1, color = "lightblue", linetype = "dashed") +  
  labs(x = "Valeurs observées", y = "Prédictions") +  
  ggtitle("Comparaison des valeurs observées et des prédictions") +  
  theme_minimal()  # Thème minimal

# Afficher le graphique
print(graphique)
```

Graphique 2

```{r}
# Filtrer les données pour inclure uniquement les valeurs entre 150 et 200
# df_0to5 <- df[df$V1 >= 0 & df$V1 <= 5h]

# create a graphe with ggplot2
graphique <- ggplot(df, aes(x = V1, y = predictions_delta_model)) +
  geom_point(shape = 4, color = "darkblue") +  
  geom_abline(intercept = 0, slope = 1, color = "lightblue", linetype = "dashed") +  
  labs(x = "Valeurs observées", y = "Prédictions") +  
  ggtitle("Comparaison des valeurs observées et des prédictions") +
  ylim(0,5)+
  xlim(0,5)+
  theme_minimal()  # Thème minimal

# Afficher le graphique
print(graphique)
```

```{r}
rmse_mod_delta <- sqrt(mean(df$V1 - df$predictions_delta_model)^2)

(rmse_mod_delta*100)/diff(range(df$V1))
```

#### Importance des variables dans le modèle gaussien (positif)

```{r}
# Calcul de l'importance des variables pour le modèle à densité positive
importance_matrix <- xgb.importance(feature_names = delta_model$features, model = delta_model$positive_model)

# Créer un graphique de l'importance des variables
ggplot(importance_matrix, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_col(fill = "steelblue") +
  coord_flip() +  # Pour rendre le graphique horizontal
  labs(title = "Importance des Variables du modèle de densité positive", x = "Variable", y = "Gain") +
  theme_minimal()

```

```{r}
# Calcul de l'importance des variables pour le modèle à densité positive
importance_matrix_logistic_model <- xgb.importance(feature_names = delta_model$features, model = delta_model$logistic_model)

# Créer un graphique de l'importance des variables
ggplot(importance_matrix_logistic_model, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_col(fill = "steelblue") +
  coord_flip() +  # Pour rendre le graphique horizontal
  labs(title = "Importance des Variables du modèle d'absence/présence", x = "Variable", y = "Gain") +
  theme_minimal()
```
