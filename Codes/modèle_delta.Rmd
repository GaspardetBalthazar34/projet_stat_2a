# Modèle delta

```{r results="hide"}
# On charge les librairies
requiredPackages = c("xgboost", "caret", "dplyr")

for(pkge in requiredPackages) {
if (!require(pkge, character.only = TRUE))
install.packages(pkge)
library(pkge, character.only = TRUE)
}
```

# MODELE BINOMIAL

## Traitement de la variable Density

```{r}
set.seed(123) # For reproductibility

# Let's reset numeric_data here just in case it has been modified somewhere else
logistic_data = data_merlan %>% select_if(is.numeric)

# Remove the Year and Month columns
logistic_data = logistic_data %>% select(-c(Surface,Abundance))

# Do not forget to extract Desnity from the dataset
logistic_data_label = unlist(logistic_data$Density)
# Transforming the density into a binary variable
logistic_data_label_binary = ifelse(test = logistic_data_label==0,0,1)
# logistic_data_label_binary = as.factor(logistic_data_label_binary)

# Supress Density from the explanatory variables
logistic_data = logistic_data %>% select(-Density)
```

## BRT

Creating the `dcgMatrix` for the boosted regression tree method `xgboost`

```{r}
# Construction of the dcgMatrix object for the dataset

dmatrix_logistic <- xgb.DMatrix(data = as.matrix(logistic_data), label = logistic_data_label_binary)
```

### Hyperparamètres

On définit les paramètres du modèle :

```{r}
params <- list(
  booster = "gbtree",
  objective = "binary:logistic", # Utilisez "reg:squarederror" pour la régression
  eta = 0.3,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = 1
)
```

#### Validation croisée

Et maintenant on lance la validation croisée

```{r}
cv.nfold <- 5 # Nombre de plis pour la validation croisée
nrounds <- 200 # Nombre d'itérations de boosting

cv_results <- xgb.cv(
  params = params,
  data = dmatrix_logistic,
  nfold = cv.nfold,
  nrounds = nrounds,
  metrics = "rmse", # Utiliser "error" pour la classification, "rmse" pour la régression, etc.
  early_stopping_rounds = 10, # arrêt précoce
  stratified = TRUE, # si la classification, pour garder la même proportion de classes dans chaque pli
  seed = 123 # pour la reproductibilité
)
```

Affichage des résultats

```{r}
print(cv_results$evaluation_log)
```

```{r}
df_rmse_binom <- data.frame(cv_results$evaluation_log)
```

```{r}
par(mfrow=c(1,2))


# Graphe of the Decrease of the RMSE
graph_rmse_binom_mean <- ggplot(df_rmse_binom, aes(x = iter, y = test_rmse_mean)) +
  geom_line(color = "darkblue") +  
  labs(x = "Nombre d'itérations", y = "RMSE") +  
  ggtitle("Décroissance de la RMSE en fonction des itérations (échantillon de test)") +  
  theme_minimal()  # Thème minimal

# Afficher le graphique
graph_rmse_binom_mean


# -----------------------------------


# Graphe of the Decrease of the std's RMSE
graph_rmse_binom_std <- ggplot(df_rmse_binom, aes(x = iter, y = test_rmse_std)) +
  geom_line(color = "darkblue") +  
  labs(x = "Nombre d'itérations", y = "RMSE") +  
  ggtitle("Ecart-type de la RMSE en fonction des itérations (échantillon de test") +  
  theme_minimal()  # Thème minimal

# Afficher le graphique
graph_rmse_binom_std

par(mfrow=c(1,1))

```

#### Opti des hyperparam

Tentative d'optimisation des paramètres :

```{r}
grid <- expand.grid(
  nrounds = c(100, 200),
  eta = c(0.01, 0.05, 0.1),
  max_depth = c(3, 6, 9),
  gamma = c(0, 0.1, 0.2),
  colsample_bytree = c(0.5, 0.75, 1),
  min_child_weight = c(1, 3, 5),
  subsample = c(0.5, 0.75, 1)
)
```

Configuration du contrôle de la formation

```{r}
control <- trainControl(method = "cv", number = 5, search = "grid")
```

### Entrainement du BRT

Entraînement du modèle (Attention ce code est très long (env. 1h d'éxécution))

```{r}
# xgb_model <- train(
#   logistic_data,
#   logistic_data_label_binary,
#   trControl = control,
#   tuneGrid = grid,
#   method = "xgbTree"
# )
```

Showing the best model :

```{r}
xgb_model$bestTune
```

Le meilleur modèle est donc le suivant :

nrounds = 200

max_depth = 6

eta = 0.05

gamma = 0.1

colsample_bytree = 1

min_child_weight = 3

subsample = 0.75

------------------------------------------------------------------------

## Modèle binom avec best hyperparam

Let's create such a model, starting with the parameters grid

```{r}
params_logistic_model <- list(
  booster = "gbtree",
  objective = "binary:logistic", # Utilisez "reg:squarederror" pour la régression
  eta = 0.05,
  gamma = 0.1,
  max_depth = 6,
  min_child_weight = 3,
  subsample = 0.75,
  colsample_bytree = 1
)
```

Training the actual logistic model for presence/absence

```{r}
logistic_model <- xgb.train(
  params = params_logistic_model,
  data = dmatrix_logistic,
  nrounds = 200, # Supposons que vous avez choisi le nombre optimal de rounds basé sur xgb.cv
  # watchlist = list(eval = dmatrix_positive_model, train = dmatrix_positive_model),
  # early_stopping_rounds = 15
)

predictions_positive <- predict(positive_model, as.matrix(positive_density_model_data))
```

```         
        A FAIRE : 
        - afficher erreur du modèle binomial (s'inspirer de celui à densité) 
   
```

# MDELE DENSITE POSITIVE

Now onto the positive density model (second part of the delta model)

## Traitement de la variable Density

```{r}
# data for the positive density model
positive_density_model_data = data_merlan %>% select_if(is.numeric)

positive_density_model_data = positive_density_model_data %>%
  filter(Density > 0)

positive_density_model_data = positive_density_model_data %>% 
  select(-c(Abundance,Surface))

positive_density_model_label = unlist(positive_density_model_data$Density)

# Remove density from the explanatory variables
positive_density_model_data = positive_density_model_data %>% 
  select(-Density)

# Assert all the remaining data has positive density
all(positive_density_model_label > 0)
```

## Premier modèle

First BRT attempt

```{r}
# Construction of the dcgMatrix object for the dataset

dmatrix_positive_model <- xgb.DMatrix(
  data = as.matrix(positive_density_model_data),
  label = positive_density_model_label)
```

params definition

```{r}
params_positive_model <- list(
  booster = "gbtree",
  objective = "reg:squarederror", # Utilisez "reg:squarederror" pour la régression
  eta = 0.05,
  gamma = 0.1,
  max_depth = 6,
  min_child_weight = 3,
  subsample = 0.75,
  colsample_bytree = 1
)
```

### Validation croisée

Cross-Validation

```{r}
cv.nfold <- 5 # Nombre de plis pour la validation croisée
nrounds <- 200 # Nombre d'itérations de boosting

cv_results_positive_model <- xgb.cv(
  params = params_positive_model,
  data = dmatrix_positive_model,
  nfold = cv.nfold,
  nrounds = nrounds,
  metrics = "rmse", # Utiliser "error" pour la classification, "rmse" pour la régression, etc.
  early_stopping_rounds = 15, # arrêt précoce
  stratified = TRUE, # si la classification, pour garder la même proportion de classes dans chaque pli
  seed = 123 # pour la reproductibilité
)
```

Showing the results of the rmse of the cross-validation :

```{r}
print(cv_results_positive_model$evaluation_log)
```

```{r}
df_rmse_density <- data.frame(cv_results_positive_model$evaluation_log)
```

```{r}
par(mfrow=c(1,2))


# Graphe of the Decrease of the RMSE
graph_rmse_dens_mean <- ggplot(df_rmse_density, aes(x = iter, y = test_rmse_mean)) +
  geom_line(color = "darkblue") +  
  labs(x = "Nombre d'itérations", y = "RMSE") +  
  ggtitle("Décroissance de la RMSE en fonction des itérations (échantillon de test)") +  
  theme_minimal()  # Thème minimal

# Afficher le graphique
graph_rmse_dens_mean


# -----------------------------------


# Graphe of the Decrease of the std's RMSE
graph_rmse_dens_std <- ggplot(df_rmse_density, aes(x = iter, y = test_rmse_std)) +
  geom_line(color = "darkblue") +  
  labs(x = "Nombre d'itérations", y = "RMSE") +  
  ggtitle("Ecart-type de la RMSE en fonction des itérations (échantillon de test") +  
  theme_minimal()  # Thème minimal

# Afficher le graphique
graph_rmse_dens_std

par(mfrow=c(1,1))

```

### Erreur du modèle à densité positive

Error visualisation:

```{r}
library(ggplot2)
```

Trying something visual

```{r}
positive_model <- xgb.train(
  params = params_positive_model,
  data = dmatrix_positive_model,
  nrounds = 200, # Supposons que vous avez choisi le nombre optimal de rounds basé sur xgb.cv
  watchlist = list(eval = dmatrix_positive_model, train = dmatrix_positive_model),
  early_stopping_rounds = 15
)

# Prédire en utilisant le modèle final
predictions_positive <- predict(positive_model, as.matrix(positive_density_model_data))
```

Onto the visual thing :

```{r}
# Créer un dataframe avec les prédictions et les valeurs réelles
results_df <- data.frame(
  Real_Density = positive_density_model_label,
  Predicted_Density = predictions_positive
)

# Afficher les premières lignes du dataframe pour vérification
head(results_df)
```

```{r}
# create a graphe with ggplot2
graphique <- ggplot(results_df, aes(x = results_df$Real_Density, y = results_df$Predicted_Density)) +
  geom_point(shape = 4, color = "lightblue") +  
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +  
  labs(x = "Valeurs observées", y = "Prédictions") +  
  ggtitle("Comparaison des valeurs observées et des prédictions") +  
  theme_minimal()  # Thème minimal

# Afficher le graphique
print(graphique)
```

Error positiv density model

```{r}
rmse_dens_pos <- sqrt(mean(results_df$Real_Density - results_df$Predicted_Density)^2)

rmse_dens_pos
```

# MODELE DELTA

## Functions !

Creating the delta model !

```{r}
train_delta_model <- function(data, target, save_model = FALSE, model_name = "delta_model"){
  library(xgboost)
  library(dplyr)
  
  # Ensure the target column is a character string
  # target <- deparse(substitute(target))
  
  # Selecting only the numeric variables and removing the target column for the logistic model
  numeric_data <- data %>%
    select_if(is.numeric) %>%
    select(-all_of(target))
  
  # Creating the target for the logistic model
  logistic_target <- ifelse(test = data[[target]]==0, 0, 1)
  
  # Creating the dataset for the Presence/Absence model, ensuring 'Density' is excluded
  positive_data <- data %>% 
    filter(!!as.name(target) > 0) %>% 
    select_if(is.numeric) %>%
    select(-all_of(target))
  
  # Creating the positive target
  positive_target <- data %>% 
    filter(!!as.name(target) > 0) %>% 
    pull(!!as.name(target))
  
  # Defining the dmatrix object for xgb.train method for the logistic model
  dmatrix_logistic <- xgb.DMatrix(
    data = as.matrix(numeric_data),
    label = logistic_target
  )
  
  # Defining the parameters for the logistic model
  params_logistic <- list(
    booster = "gbtree",
    objective = "binary:logistic",
    eta = 0.05,
    gamma = 0.1,
    max_depth = 6,
    min_child_weight = 3,
    subsample = 0.75,
    colsample_bytree = 1
  )
  
  # Training the logistic model
  logistic_model <- xgb.train(
    params = params_logistic,
    data = dmatrix_logistic,
    nrounds = 200
  )
  
  # Parameters for the positive model
  params_positive <- list(
    booster = "gbtree",
    objective = "reg:squarederror",
    eta = 0.05,
    gamma = 0.1,
    max_depth = 6,
    min_child_weight = 3,
    subsample = 0.75,
    colsample_bytree = 1
  )
  
  # Defining the dmatrix object for xgb.train method for the positive model
  dmatrix_positive <- xgb.DMatrix(
    data = as.matrix(positive_data),
    label = positive_target
  )
  
  # Training the positive model
  positive_model <- xgb.train(
    params = params_positive,
    data = dmatrix_positive,
    nrounds = 200
  )
  
  # Include the feature names in the return object
  model_features <- names(numeric_data)
  
  if(save_model == TRUE){
    saveRDS(
      list(
        logistic_model = logistic_model,
        positive_model = positive_model,
        features = model_features
      ),
      paste0("../Models/", model_name, ".rds")
    )
  }
  
  # Return both models
  return(
    list(
      logistic_model = logistic_model,
      positive_model = positive_model,
      features = model_features
    )
  )
}
```

Now, let's create the **`predict_delta`**saveRDS( list(**`_model`** function. This function will accept a trained model (from **`train_delta_model`**) and new data to make predictions:

```{r}
predict_delta_model <- function(model, newdata) {
  library(xgboost)
  library(dplyr)
  
  # Use the features stored in the model object
  model_features <- model$features
  
  # Ensure newdata contains only the features used during training
  newdata_numeric <- newdata %>% 
    select(all_of(model_features)) # Use model_features to select features
  
  # DMatrix object for the prediction
  dmatrix_newdata <- xgb.DMatrix(data = as.matrix(newdata_numeric))

  # Predict presence/absence using the logistic model
  presence_probabilities <- predict(model$logistic_model, dmatrix_newdata)
  
  # Initialize predictions vector
  final_predictions <- rep(0, nrow(newdata))
  
  # Predict with the positive model where presence is likely
  positive_indices <- which(presence_probabilities > 0.5)
  if (length(positive_indices) > 0) {
    # Ensure feature selection for positive prediction matches training
    positive_newdata <- newdata_numeric[positive_indices, , drop = FALSE]
    dmatrix_positive_newdata <- xgb.DMatrix(data = as.matrix(positive_newdata))
    positive_predictions <- predict(model$positive_model, dmatrix_positive_newdata)
    final_predictions[positive_indices] <- positive_predictions
  }
  
  return(final_predictions)
}

```

```{r}
set.seed(123) # For reproductibility

# Remove the Surface and Abundance columns
model_data = data_merlan %>% select(-c(Surface,Abundance))

delta_model <- train_delta_model(
  data = model_data,
  target = "Density",
  save_model = TRUE
)

prediction_data = model_data %>% select(-Density)

predictions_delta_model <- predict_delta_model(
  model = delta_model,
  newdata = prediction_data
)
```

## Visualization

Just to visualize :

```{r}
df = cbind(data_merlan$Density,predictions_delta_model)

# Transform df in a real df 
df <- data.frame(df)
View(df)
```

Error visualisation:

```{r}
library(ggplot2)
```

```{r}
# create a graphe with ggplot2
graphique <- ggplot(df, aes(x = V1, y = predictions_delta_model)) +
  geom_point(shape = 4, color = "darkblue") +  
  geom_abline(intercept = 0, slope = 1, color = "lightblue", linetype = "dashed") +  
  labs(x = "Valeurs observées", y = "Prédictions") +  
  ggtitle("Comparaison des valeurs observées et des prédictions") +  
  theme_minimal()  # Thème minimal

# Afficher le graphique
print(graphique)
```

```{r}
rmse_mod_delta <- sqrt(mean(df$V1 - df$predictions_delta_model)^2)

(rmse_mod_delta*100)/range(diff(df$V1))
```
