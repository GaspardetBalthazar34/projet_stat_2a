# Modèle delta

```{r}
# On charge les librairies

require(xgboost)
require(caret)
require(dplyr)
```

Ballbla

```{r}
set.seed(123) # For reproductibility

# Let's reset numeric_data here just in case it has been modified somewhere else
final_data = data_merlan %>% select_if(is.numeric)

# Remove the Year and Month columns
final_data = final_data %>% select(-c(Surface,Abundance))

# Do not forget to extract Desnity from the dataset
final_data_label = unlist(final_data$Density)
# Transformation pour le 
final_data_label_binary = ifelse(test = final_data_label==0,0,1)
final_data_label_binary = as.factor(final_data_label_binary)

# Supress Density from the explanatory variables
final_data = final_data %>% select(-Density)
```

Creating the `dcgMatrix` for the boosted regression tree method `xgboost`

```{r}
# Construction of the dcgMatrix object for the dataset

dmatrix <- xgb.DMatrix(data = as.matrix(final_data), label = final_data_label_binary)
```

On définit les paramètres du modèle :

```{r}
params <- list(
  booster = "gbtree",
  objective = "binary:logistic", # Utilisez "reg:squarederror" pour la régression
  eta = 0.3,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = 1
)
```

Et maintenant on lance la validation croisée

```{r}
cv.nfold <- 5 # Nombre de plis pour la validation croisée
nrounds <- 100 # Nombre d'itérations de boosting

cv_results <- xgb.cv(
  params = params,
  data = dmatrix,
  label = labels,
  nfold = cv.nfold,
  nrounds = nrounds,
  metrics = "rmse", # Utiliser "error" pour la classification, "rmse" pour la régression, etc.
  early_stopping_rounds = 10, # arrêt précoce
  stratified = TRUE, # si la classification, pour garder la même proportion de classes dans chaque pli
  seed = 123 # pour la reproductibilité
)
```

Affichage des résultats

```{r}
print(cv_results$evaluation_log)
```

Tentative d'optimisation des paramètres :

```{r}
grid <- expand.grid(
  nrounds = c(100, 200),
  eta = c(0.01, 0.05, 0.1),
  max_depth = c(3, 6, 9),
  gamma = c(0, 0.1, 0.2),
  colsample_bytree = c(0.5, 0.75, 1),
  min_child_weight = c(1, 3, 5),
  subsample = c(0.5, 0.75, 1)
)
```

Configuration du contrôle de la formation

```{r}
control <- trainControl(method = "cv", number = 5, search = "grid")
```

Entraînement du modèle (Attention ce code est très long (env. 1h d'éxécution))

```{r}
xgb_model <- train(
  final_data,
  final_data_label_binary,
  trControl = control,
  tuneGrid = grid,
  method = "xgbTree"
)
```

Showing the best model :

```{r}
xgb_model$bestTune
```

Le meilleur modèle est donc le suivant :

nrounds = 200

max_depth = 6

eta = 0.05

gamma = 0.1

colsample_bytree = 1

min_child_weight = 3

subsample = 0.75

Now onto the positive density model (second part of the delta model)

```{r}
# data for the positive density model
positive_density_model_data = data_merlan %>% select_if(is.numeric)

positive_density_model_data = positive_density_model_data %>%
  filter(Density > 0)

positive_density_model_data = positive_density_model_data %>% 
  select(-c(Abundance,Surface))

positive_density_model_label = unlist(positive_density_model_data$Density)

# Remove density from the explanatory variables
positive_density_model_data = positive_density_model_data %>% 
  select(-Density)

# Assert all the remaining data has positive density
all(positive_density_model_label > 0)
```

First BRT attempt

```{r}
# Construction of the dcgMatrix object for the dataset

dmatrix_positive_model <- xgb.DMatrix(
  data = as.matrix(positive_density_model_data),
  label = positive_density_model_label)
```

params definition

```{r}
params_positive_model <- list(
  booster = "gbtree",
  objective = "reg:squarederror", # Utilisez "reg:squarederror" pour la régression
  eta = 0.05,
  gamma = 0.1,
  max_depth = 6,
  min_child_weight = 3,
  subsample = 0.75,
  colsample_bytree = 1
)
```

training the model

```{r}
cv.nfold <- 5 # Nombre de plis pour la validation croisée
nrounds <- 200 # Nombre d'itérations de boosting

cv_results_positive_model <- xgb.cv(
  params = params_positive_model,
  data = dmatrix_positive_model,
  label = labels,
  nfold = cv.nfold,
  nrounds = nrounds,
  metrics = "rmse", # Utiliser "error" pour la classification, "rmse" pour la régression, etc.
  early_stopping_rounds = 15, # arrêt précoce
  stratified = TRUE, # si la classification, pour garder la même proportion de classes dans chaque pli
  seed = 123 # pour la reproductibilité
)
```

Afficahge des résultats

```{r}
print(cv_results_positive_model$evaluation_log)
```

Trying something visual

```{r}
model_final <- xgb.train(
  params = params_positive_model,
  data = dmatrix_positive_model,
  nrounds = 200, # Supposons que vous avez choisi le nombre optimal de rounds basé sur xgb.cv
  watchlist = list(eval = dmatrix_positive_model, train = dmatrix_positive_model),
  early_stopping_rounds = 15
)

# Prédire en utilisant le modèle final
predictions_final <- predict(model_final, as.matrix(positive_density_model_data))
```

Onto the visual thing :

```{r}
# Créer un dataframe avec les prédictions et les valeurs réelles
results_df <- data.frame(
  Real_Density = positive_density_model_label,
  Predicted_Density = predictions_final
)

# Afficher les premières lignes du dataframe pour vérification
head(results_df)
```
